\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

\begin{center}
	\vspace{5mm}
	\MakeUppercase{\textbf{Realtime Multi-Object Tracking and Pixelwise Segmentation}}\\
	\vspace{5mm}
	Group Members: \memberA, \memberB, \memberC, \memberD \\
	\vspace{5mm}
	Supervisors: \supervisorA, \supervisorB \\
	\vspace{5mm}
\end{center}

\noindent Keywords: Vision, Perception, Detection, Tracking, Panoptic Segmentation, Siamese Network, Conditional Random Field, Recurrent Neural Network, Autonomous Systems. \\

Bleeding-edge technological pursuits ranging from self-guided robots at the research stage to mass scale industrial applications such as augmented reality, intelligent security systems and self-driving vehicles heavily rely on perception through vision. Vision based perception of the environment in autonomous systems extensively use object detection, segmentation and more importantly tracking as fundamental components. Despite the recent advancements in deep learning-based object detection on monocular images, the several highly publicized accidents involving self-driving vehicles and critical failures in monitoring and navigation systems highlight the need for significant further improvement for real-time tracking systems in critical applications. We identify two such key areas of improvement and introduce two separate novel frameworks to tackle each problem. 

Firstly, we observe that trackers often perform underwhelmingly in object dense situations where occlusions and crossovers are prevalent. We identify that in order to perform better in these scenarios both appearance and motion information should be incorporated. Siamese networks have recently become highly successful at appearance based single object tracking while Recurrent Neural Networks (RNNs) have started dominating motion-based tracking. Our work focuses on combining Siamese networks and RNNs to exploit both (temporally varying) appearance and motion information to build a robust framework that can also operate in real-time. We further explore heuristics-based constraints for tracking in the Birds Eye View Space for efficiently exploiting 3D information as a constrained optimization problem for track prediction. 

Our second observation is that most trackers lack precise (pixel-level) awareness of both object classes (countable) and back-ground classes in a frame. This is known as panoptic segmentation. We tackle the panoptic segmentation problem with a conditional random field (CRF) model. Panoptic segmentation involves assigning a semantic label and an instance label to each pixel of a given image. At each pixel, the semantic label and the instance label should be compatible. Furthermore, a good panoptic segmentation should have several other desirable properties such as the spatial and colour consistency of the labelling (similar looking neighbouring pixels should have the same semantic label and the instance label). To tackle this problem, we propose a CRF model, named Bipartite CRF or BCRF, with two types of random variables for semantic and instance labels. In this formulation, various energies are defined within and across the two types of random variables to encourage a consistent panoptic segmentation. We propose a mean-field-based efficient inference algorithm for solving the CRF and empirically show its convergence properties. This algorithm is fully differentiable, and therefore, BCRF inference can be included as a trainable module in a deep network. In the experimental evaluation, we quantitatively and qualitatively show that the BCRF yields superior panoptic segmentation results in practice.

We integrate both these components in a joint tracking framework that is suitable for densely populated real world environments which are inherently chaotic, specifically in the domains of autonomous driving and fully automated stores.
